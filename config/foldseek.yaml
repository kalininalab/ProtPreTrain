seed_everything: 42
trainer:
  logger:
    class_path: WandbLogger
    init_args:
      project: step
      log_model: all
      dir: wandb
      tags: [model0, test]
  enable_checkpointing: true
  # profiler: pytorch
  callbacks:
    - class_path: ModelCheckpoint
      init_args:
        monitor: train/loss
        mode: min
        every_n_train_steps: 1000
        dirpath: checkpoints
    - class_path: RichProgressBar
    - class_path: RichModelSummary
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: step
  max_epochs: 10000
  log_every_n_steps: 100
  accelerator: gpu
  precision: bf16-mixed
model:
  class_path: step.models.DenoiseModel
  init_args:
    hidden_dim: 512
    pe_dim: 128
    pos_dim: 128
    num_layers: 12
    attn_type: performer
    dropout: 0.5
    alpha: 1.0
    predict_all: true
data:
  class_path: FoldSeekDataModule
  init_args:
    pre_transforms:
      - class_path: torch_geometric.transforms.Center
      - class_path: torch_geometric.transforms.NormalizeRotation
    transforms:
      - class_path: step.data.PosNoise
        init_args:
          sigma: 1.0
          plddt_dependent: false
      - class_path: step.data.MaskType
        init_args:
          pick_prob: 0.2
      - class_path: torch_geometric.transforms.RadiusGraph
        init_args:
          r: 7.0
      - class_path: torch_geometric.transforms.ToUndirected
      - class_path: step.data.transforms.RandomWalkPE
        init_args:
          walk_length: 20
          attr_name: pe
    num_workers: 10
    shuffle: true
    batch_sampling: true
    # batch_size: 4
    max_num_nodes: 4096
optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 1.0e-4
lr_scheduler:
  class_path: step.utils.WarmUpCosineLR
  init_args:
    warmup_steps: 10000
    start_lr: 1.0e-5
    max_lr: 1.0e-4
    min_lr: 1.0e-7
    cycle_len: 100000
