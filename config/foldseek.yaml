seed_everything: 42
trainer:
  logger:
    class_path: WandbLogger
    init_args:
      name: foldseek
      project: step
      log_model: true
      dir: wandb
  enable_checkpointing: true
  callbacks:
    - class_path: ModelCheckpoint
      init_args:
        monitor: train/loss
        mode: min
    - class_path: RichProgressBar
    - class_path: RichModelSummary
    - class_path: LearningRateMonitor
  accumulate_grad_batches: 4
  max_epochs: 10000
  log_every_n_steps: 50
  accelerator: gpu
model:
  class_path: DenoiseModel
  init_args:
    local_module: GAT
    global_module: Performer
    hidden_dim: 1024
    num_layers: 24
    num_heads: 16
    dropout: 0.1
    attn_dropout: 0.1
    alpha: 1.0
    weighted_loss: false
data:
  class_path: FoldSeekDataModule
  init_args:
    pre_transforms:
      - class_path: torch_geometric.transforms.Center
      - class_path: torch_geometric.transforms.NormalizeRotation
    transforms:
      - class_path: step.data.PosNoise
        init_args:
          sigma: 1.0
          plddt_dependent: false
      - class_path: torch_geometric.transforms.RadiusGraph
        init_args:
          r: 7.0
      - class_path: torch_geometric.transforms.ToUndirected
      - class_path: torch_geometric.transforms.Spherical
      - class_path: step.data.MaskType
        init_args:
          pick_prob: 0.2
    num_workers: 16
    shuffle: true
    batch_sampling: true
    max_num_nodes: 1024
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1.0e-4
lr_scheduler:
  class_path: cosine_annealing_warmup.CosineAnnealingWarmupRestarts
  init_args:
    first_cycle_steps: 1000
    cycle_mult: 1.0
    max_lr: 1.0e-4
    min_lr: 1.0e-6
    warmup_steps: 100
    gamma: 1.0
