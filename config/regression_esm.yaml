seed_everything: true
trainer:
  enable_checkpointing:
  logger:
    class_path: WandbLogger
    init_args:
      log_model: true
      project: step
      dir: wandb
      tags: [esm]
  callbacks:
    - class_path: ModelCheckpoint
      init_args:
        monitor: val/loss
        mode: min
        dirpath: logs
    - class_path: RichProgressBar
    - class_path: RichModelSummary
    - class_path: LearningRateMonitor
    - class_path: EarlyStopping
      init_args:
        monitor: val/loss
        patience: 20
        mode: min
  gradient_clip_val: 0.0
  accelerator: gpu
  devices: -1
  max_epochs: 10000
  num_sanity_val_steps: 2
  fast_dev_run: false
model:
  class_path: RegressionESMModel
  init_args:
    hidden_dim: 1024
    dropout: 0.3
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1.0e-3
# lr_scheduler:
#   class_path: cosine_annealing_warmup.CosineAnnealingWarmupRestarts
#   init_args:
#     first_cycle_steps: 1000
#     cycle_mult: 1.0
#     max_lr: 1.0e-3
#     min_lr: 1.0e-5
#     warmup_steps: 100
#     gamma: 1.0
lr_scheduler:
  class_path: ReduceLROnPlateau
  init_args:
    mode: min
    monitor: val/loss
    factor: 0.1
    patience: 10
    min_lr: 1.0e-7
    verbose: true
data:
  # class_path: StabilityDataModule
  init_args:
    num_workers: 16
    shuffle: true
    batch_sampling: false
    batch_size: 1024
    # max_num_nodes: 1024
